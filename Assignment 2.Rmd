---
title: "Assignment 2"
author: "Charvi Mittal,Monika Lodha, Ashwin baabu Paramasivan"
date: "August 18, 2017"
output: github_document
---
```{r,warning=FALSE}
```

## Q1.Flights at ABIA

Consider the data in ABIA.csv, which contains information on every commercial flight in 2008 that either departed from or landed at Austin-Bergstrom Interational Airport. The variable codebook is as follows:

Year all 2008
Month 1-12
DayofMonth 1-31
DayOfWeek 1 (Monday) - 7 (Sunday)
DepTime actual departure time (local, hhmm)
CRSDepTime scheduled departure time (local, hhmm)
ArrTime actual arrival time (local, hhmm)
CRSArrTime scheduled arrival time (local, hhmm)
UniqueCarrier unique carrier code
FlightNum flight number
TailNum plane tail number
ActualElapsedTime in minutes
CRSElapsedTime in minutes
AirTime in minutes
ArrDelay arrival delay, in minutes
DepDelay departure delay, in minutes
Origin origin IATA airport code
Dest destination IATA airport code
Distance in miles
TaxiIn taxi in time, in minutes
TaxiOut taxi out time in minutes
Cancelled was the flight cancelled?
CancellationCode reason for cancellation (A = carrier, B = weather, C = NAS, D = security)
Diverted 1 = yes, 0 = no
CarrierDelay in minutes
WeatherDelay in minutes
NASDelay in minutes
SecurityDelay in minutes
LateAircraftDelay in minutes
Your task is to create a figure, or set of related figures, that tell an interesting story about flights into and out of Austin. You can annotate the figure and briefly describe it, but strive to make it as stand-alone as possible. It shouldn't need many, many paragraphs to convey its meaning. Rather, the figure should speak for itself as far as possible. For example, you might consider one of the following questions:

What is the best time of day to fly to minimize delays?
What is the best time of year to fly to minimize delays?
How do patterns of flights to different destinations or parts of the country change over the course of the year?
What are the bad airports to fly to?
But anything interesting will fly.


#####Reading data
```{r}
library(data.table)
library(ggplot2)
Austin_flight = read.csv("ABIA.csv")
```
##Let's find out the month with the highest fraction of delays
```{r}
#Replacing numerical month indicators to month names
Austin_flight$Month<-factor(Austin_flight$Month,levels=c(1,2,3,4,5,6,7,8,9,10,11,12),
            labels=c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"))

#Selecting required columns and removing NA values
Austin_flight1<-Austin_flight[,c(2,16)]
Austin_flight1=Austin_flight1[which(!(is.na(Austin_flight1$DepDelay))),]

# According to FAA, a flight is considered delayed if it's delayed by more than 15 minutes.
Austin_flight1['Status']<-ifelse(Austin_flight1$DepDelay <= 15,'Undelayed','Delayed')

#Creating a dataframe with fraction of flight delays. Considering fraction because it might be the case that a particular month might have more number of total flights. Hence, number of delayed flights is not a fair measure.
Austin_flight2<-Austin_flight1[,c(1,3)]
set_m = table(Austin_flight2$Month,Austin_flight2$Status)
set_month<-as.data.frame.matrix(set_m)
set_month['fraction'] = set_month$Delayed/(set_month$Delayed+set_month$Undelayed)
setDT(set_month, keep.rownames = TRUE)[]
set_month[,'fraction']=round(set_month[,'fraction'],2)

#Plotting fraction of flight delays against months
theme_set(theme_bw())
ggplot(set_month, aes(x=rn, y=fraction)) + 
  geom_point(size=3) + 
  geom_segment(aes(x=rn, 
                   xend=rn, 
                   y=0, 
                   yend=fraction)) + 
  labs(x = "Month") +
  labs(y = "Fraction of delayed flights") + 
  geom_text(aes(label=fraction),hjust=0, vjust=-0.5) +
  labs(title="Lollipop Chart", 
       subtitle="Fraction of delay for months", 
       caption="source: flights") + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6))
```
  
###25% of the flights in december gets delayed.  

##So which is the best month to travel to minimize delays?  
###The best month to travel is in the months of September, October and November.  
####The worst month to travel is March and December which makes sense because December includes travel for the holidays and March includes travel for Spring Break. These time periods typically see an influx of travellers, which could be a factor contributing to an increase in fraction of flight delays.

##Next let's see the pattern of delay in a week
```{r}
#Replacing numerical day indicators to day names
Austin_flight$DayOfWeek<-factor(Austin_flight$DayOfWeek,levels=c(1,2,3,4,5,6,7),
                                labels=c("Mon","Tue","Wed","Thu","Fri","Sat","Sun"))

#Selecting required columns and removing NA values
Austin_flight1<-Austin_flight[,c(4,16)]
Austin_flight1=Austin_flight1[which(!(is.na(Austin_flight1$DepDelay))),]

# According to FAA, a flight is considered delayed if it's delayed by more than 15 minutes.
Austin_flight1['Status']<-ifelse(Austin_flight1$DepDelay <= 15,'Undelayed','Delayed')

#Creating a dataframe with fraction of flight delays. Considering fraction because it might be the case that a particular month might have more number of total flights. Hence, number of delayed flights is not a fair measure.
Austin_flight2<-Austin_flight1[,c(1,3)]
set_m = table(Austin_flight2$DayOfWeek,Austin_flight2$Status)
set_day<-as.data.frame.matrix(set_m)
set_day['fraction'] = set_day$Delayed/(set_day$Delayed+set_day$Undelayed)
setDT(set_day, keep.rownames = TRUE)[]
set_day[,'fraction']=round(set_day[,'fraction'],2)

#Plotting fraction of flight delays against days of the week
theme_set(theme_bw())
ggplot(set_day, aes(x=rn, y=fraction)) + 
  geom_point(size=3) + 
  geom_segment(aes(x=rn, 
                   xend=rn, 
                   y=0, 
                   yend=fraction)) + 
  labs(x = "Day of the week") +
  labs(y = "Fraction of delayed flights") + 
  geom_text(aes(label=fraction),hjust=0, vjust=-0.5) +
  labs(title="Lollipop Chart", 
       subtitle="Fraction of delay for days of the week", 
       caption="source: flights") + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6))
```
  
###21% of the flights gets delayed on a Friday  

##So which is the best day to travel to minimize delays?  
###The best day to travel is Wednesday and Saturday.  

##Finally let's find out the busiest time of the day
```{r}
#Selecting necessary columns
Austin_flight1<-Austin_flight[,c(6,16)]
#Removing NA values
Austin_flight1=Austin_flight1[which(!(is.na(Austin_flight1$DepDelay))),]
#Putting all departure times into bins
Austin_flight1['Departure_time']<-ifelse(Austin_flight1$CRSDepTime > 100 & Austin_flight1$CRSDepTime <= 500,'1am-5am',
                                  ifelse(Austin_flight1$CRSDepTime > 500 & Austin_flight1$CRSDepTime <= 900,'5am-9am',
                                  ifelse(Austin_flight1$CRSDepTime > 900 & Austin_flight1$CRSDepTime <= 1300,'9am-1pm',
                                  ifelse(Austin_flight1$CRSDepTime > 1300 & Austin_flight1$CRSDepTime <= 1700,'1pm-5pm',
                                  ifelse(Austin_flight1$CRSDepTime > 1700 & Austin_flight1$CRSDepTime <= 2100,'5pm-9pm','9pm-1am')))))

# According to FAA, a flight is considered delayed if it's delayed by more than 15 minutes

Austin_flight1['Status']<-ifelse(Austin_flight1$DepDelay <= 15,'Undelayed','Delayed')

#Creating a dataframe with fraction of flight delays. Considering fraction because it might be the case that a particular time bin might have more number of total flights. Hence, number of delayed flights is not a fair measure.

Austin_flight2<-Austin_flight1[,c(3,4)]
set_a = table(Austin_flight2$Departure_time,Austin_flight2$Status)
set_time<-as.data.frame.matrix(set_a)
View(set_a)
set_time['fraction'] = set_time$Delayed/(set_time$Delayed+set_time$Undelayed)
setDT(set_time, keep.rownames = TRUE)[]
set_time[,'fraction']=round(set_time[,'fraction'],2)
View(set_time)

#Plotting fraction of flight delays against time bins
library(ggplot2)
library(scales)
theme_set(theme_classic())

ggplot(set_time, aes(x=rn,y=fraction))+ 
  geom_point(col="tomato2", size=3) +   
  # Draw points
  geom_segment(aes(x=rn, 
                   xend=rn,
                   y=min(fraction), 
                   yend=max(fraction)), 
               linetype="dashed", 
               size=0.1)+   
  labs(x = "Time bins") +
  labs(y = "Fraction of delayed flights") + 
  geom_text(aes(label=fraction),hjust=0, vjust=-0.5) +
  # Draw dashed lines
  labs(title="Dot Plot", 
       subtitle="Fraction of delay for time bins", 
       caption="source: flights") +  
  coord_flip()
```
  
###We see that 32% of flights get delayed if they are departing from Austin between 9pm and 1am.  
##So what is the best time to travel in a day to minimize delays?  
###The best time to travel is between 5am and 9am.  


####These fractions include only the departure delay times from the dataset due to several reasons. Because the other delay variables (Weather, Security, etc. ) had a very large number of missing values and hence they were excluded from the delay computations in order to use as much of the dataset as possible. For example, the Security Delay variable or WeatherDelay had 79,513 NA values (80% of the total dataset) so including it would not significantly affect delay time.



## Q2.Author attribution

Revisit the Reuters C50 corpus that we explored in class. Your task is to build two separate models (using any combination of tools you see fit) for predicting the author of an article on the basis of that article's textual content. Describe clearly what models you are using, how you constructed features, and so forth. (Yes, this is a supervised learning task, but it potentially draws on a lot of what you know about unsupervised learning!)

In the C50train directory, you have ~50 articles from each of 50 different authors (one author per directory). Use this training data (and this data alone) to build the two models. Then apply your model to the articles by the same authors in the C50test directory, which is about the same size as the training set. How well do your models do at predicting the author identities in this out-of-sample setting? Are there any sets of authors whose articles seem difficult to distinguish from one another? Which model do you prefer?

Note: you will need to figure out a way to deal with words in the test set that you never saw in the training set. This is a nontrivial aspect of the modeling exercise.

#### Library Load
```{r}
library(tm) 
library(magrittr)
library(e1071)
library(caret)
library(dplyr)
library(doParallel)
library(foreach)
library(randomForest)
library(plyr)
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }
```


############ TRAIN DATASET
```{r}
author_dirs = Sys.glob('ReutersC50/C50train/*')
```


#### Extract authors, file names
```{r}
file_list = NULL
train_labels = NULL
for(author in author_dirs) {
  author_name = substring(author, first=21)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list = append(file_list, files_to_add)
  train_labels = append(train_labels, rep(author_name, length(files_to_add)))
}

all_docs = lapply(file_list, readerPlain)

mynames = file_list %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
  unlist

names(all_docs) = mynames
names(all_docs) = sub('.txt', '', names(all_docs))
```


### Create a corpus
```{r}
train_Corpus = Corpus(VectorSource(all_docs))
```


# Preprocessing
```{r}
train_Corpus = tm_map(train_Corpus, content_transformer(tolower)) # make everything lowercase
train_Corpus = tm_map(train_Corpus, content_transformer(removeNumbers)) # remove numbers
train_Corpus = tm_map(train_Corpus, content_transformer(removePunctuation)) # remove punctuation
train_Corpus = tm_map(train_Corpus, content_transformer(stripWhitespace)) ## remove excess white-space
train_Corpus = tm_map(train_Corpus, content_transformer(removeWords), stopwords("SMART"))
```


### Create document term matrix
```{r}
DTM_train = DocumentTermMatrix(train_Corpus,control=list(weighting=weightTfIdf, bounds = list(global = c(5, Inf))))
```


#### Keep terms which appear in 95% or more documents
```{r}
DTM_train = removeSparseTerms(DTM_train, 0.95)
DTM_train
DTM_train = as.matrix(DTM_train)
DTM_train = as.data.frame(DTM_train)
```

#### TEST Dataset
### Repeat steps as above to get document matrix for test dataset
```{r}
author_dirs = Sys.glob('ReutersC50/C50test/*')

file_list = NULL
test_labels = NULL
author_names = NULL
for(author in author_dirs) {
  author_name = substring(author, first=20)
  author_names = append(author_names, author_name)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list = append(file_list, files_to_add)
  test_labels = append(test_labels, rep(author_name, length(files_to_add)))
}

all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))
```

### test corpus
```{r}
test_corpus = Corpus(VectorSource(all_docs))

test_corpus = tm_map(test_corpus, content_transformer(tolower))
test_corpus = tm_map(test_corpus, content_transformer(removeNumbers))
test_corpus = tm_map(test_corpus, content_transformer(removePunctuation))
test_corpus = tm_map(test_corpus, content_transformer(stripWhitespace))
test_corpus = tm_map(test_corpus, content_transformer(removeWords), stopwords("en"))

DTM_test = DocumentTermMatrix(test_corpus, control=list(weighting=weightTfIdf, bounds = list(global = c(5, Inf))))
DTM_test = removeSparseTerms(DTM_test, 0.95)
DTM_test
DTM_test = as.matrix(DTM_test)
DTM_test = as.data.frame(DTM_test)
```

#### Model building for Naive Bayes

### Consider all features and append corresponding author names for each document
```{r}
DTM_train<-cbind(DTM_train,train_labels)
DTM_test<-cbind(DTM_test,test_labels)

nB = naiveBayes(as.factor(train_labels)~., data=DTM_train)
nB_predictions = predict(nB, DTM_test[,-ncol(DTM_test)], type="class")
```

#### Comparison of predictions and actual authors
```{r}
nB_table <-data.frame(table(DTM_test$test_labels, nB_predictions))

sum(DTM_test$test_labels == nB_predictions)
```

### Around 1000 authors matched out of a total 2500 author-document combinations
```{r}
nB_table %>% filter(Var1 == nB_predictions) %>% arrange(desc(Freq)) %>% filter(Freq >= 25)
```

### Most matched author is LynnleyBrowning with match of 38


#### #### check for DavidLawder with 4 matches. Which author did he mostly match with?
```{r}
nB_table %>% filter(Var1 == 'DavidLawder') %>% filter(Freq >0)
```

### His writing style matched with ToddNissen


#### Let's check for authors with same writing style and hence tough to figure the author attribution
```{r}
nB_table %>% filter(Var1 != nB_predictions) %>% filter(Freq >10) %>% arrange(desc(Freq))
```


#### How accurate was this model?
```{r}
confusionMatrix(DTM_test$test_labels, nB_predictions)$overall['Accuracy']
```


##### Accuracy of 40% with naive bayes

##### Model building for Random Forests
```{r}
set.seed(100)

registerDoParallel(cores = 6)
testTrees = c(10,50,75,100,200,400)
TreeClass = foreach( i = 1:length(testTrees),.combine = 'c') %dopar%
{
  model_RF = randomForest::randomForest(x=DTM_train[,-ncol(DTM_train)], y=as.factor(train_labels),ntree = testTrees[i])
  pred_RF = predict(model_RF, data=DTM_test[,-ncol(DTM_test)])
  caret::confusionMatrix(DTM_test$test_labels, pred_RF)$overall['Accuracy']
}
```


```{r}
plot(testTrees,TreeClass,type = "l", col = "red",main = "Random Forest",xlab = "Number of Trees",ylab = "Accuracy")
```


### Doubling trees from 200 to 400 increases accuracy only by about 3-4%
### hence we consider tree size of 200 so that model is too complex
#### Random Forest with trees as 200 
```{r}
model_RF = randomForest(x=DTM_train[,-ncol(DTM_train)], y=as.factor(train_labels), ntree=200)
plot(model_RF)
pred_RF = predict(model_RF, data=DTM_test[,-ncol(DTM_test)])
```

### Comparison of actual and predicted values
```{r}
RF_table <-data.frame(table(DTM_test$test_labels, pred_RF))
```


```{r}
sum(DTM_test$test_labels == pred_RF)
```

### There was a match of 1906 author-document which is pretty high when compared with Naive Bayes

```{r}
RF_match<-data.frame(DTM_test$test_labels[which(DTM_test$test_labels == pred_RF)])
```


#### Top matched authors
```{r}
count(RF_match,RF_match$Labels)[1:10,1:2]
```

#### check for William Kazer with 22 matches. Which author did he mostly match with?
```{r}
RF_table %>% filter(Var1 == 'WilliamKazer') %>% filter(Freq >0)
```

#### william Kazer matched with Mure Dickie and Benjamin KangLim 6 and 5 times repectively
### This number is quite low. This shows Random Forest is a good model for predicting author attributions


#### Let's check for authors with same writing style and hence tough to figure the author attribution
```{r}
RF_table %>% filter(Var1 != pred_RF) %>% filter(Freq >5) %>% arrange(desc(Freq))
```


#### Accuracy of Random Forest

```{r}
confusionMatrix(DTM_test$test_labels, pred_RF)$overall['Accuracy']
```

#### Accuracy of 76% with Random Forests


### Random Forest performs better in this scenario with an accuracy of 76% over an accuracy of 40% 
### associated with Naive Bayes. 
### Naive Bayes had author match of around 1000 with most matched author being Lynnley Browning
### with a match of 38. Most mismatched author was David Lawder with just 4 matches. his writing style
### mostly matched with Todd Nissen

### Random Forests has author match of 1906 with Aaron Pressman being highest match of 45.
### Most mismatched author was William Kazer with 22 match and he had 5 and 6 matches with Benjamin
### Kanglim and Mure Dickie 
### This shows how Random Forests has improved. 
### also the highest match in with Naive Bayes 'Lynnley Browing' now has a match of 46



## Q3. Practice with association rule mining

Revisit the notes on association rule mining, and walk through the R example on music playlists: playlists.R and playlists.csv. Then use the data on grocery purchases in groceries.txt and find some interesting association rules for these shopping baskets. The data file is a list of baskets: one row per basket, with multiple items per row separated by commas -- you'll have to cobble together a few utilities for processing this into the format expected by the "arules" package. Pick your own thresholds for lift and confidence; just be clear what these thresholds are and how you picked them. Do your discovered item sets make sense? Present your discoveries in an interesting and concise way.


```{r}
library(arules)
library(arulesViz)
groceries <- read.transactions(file="groceries.txt",sep = ',',format="basket",rm.duplicates=TRUE)
summary(groceries)
```
```{r}
itemFrequencyPlot(groceries, topN=25, type='absolute')
```



#### As seen from the plot the frequency of occurence of certain items(WHole milk, other vegetables, rolls/buns, soda and yogurt) have frequency of occurences more than 1000, hecne it also means that they could possibly play an important role in the rule mapping.


```{r}
grocrules <- apriori(groceries, 
                      parameter=list(support=.001, confidence=0.90, maxlen=40))
inspect(grocrules)

```

#### This finds all association rule mappings(129 rules in this case) that have a support of atleast 0.001 and confidence of 0.90. Here we are taking a support of lower value and higher confidence, this indicates that the combination or rule mapping occurs in 0.1%(0.001)  of the transactions. 0.1% is considered a good enough number for support as the overall dataset size is high (about 10000). We are taking a higher confidence as it indicates 90% probablity that these assocaitions occur in the basket. Setting maxlen very high as we want to consider all possible items in the basket.Lift indicates the possibility that a given item in rhs will occur if a given set of item in lhs occur.


```{r}
plot(grocrules)
```


#### As seen from the plot the lift value keeps decreasing as we increase the support and confidence, in  a parabolic fashion. This indicates that we need to take the right mix of the confidence, support and lift to identify the best rule mappings.


```{r}
inspect(subset(grocrules,subset=lift>5))
```


#### Here we specify the lift value as greater than 5, which indicates that there is 5 times more chances of RHS occuring if LHS occurs, which is a significantly hihg lift value. There are 15 such rules which are a subseet of the initial rules we had mapped, which briefly indicates that there is a higher chances of other vegetables occuring in combination with tropical fruit, citrus fruit, whipped/sour cream and similarly root vegetables, yogurt occurs with higher chances if butter, sliced cheese, cream cheese occur. We will summarise this in a little more in detail at the end.



```{r}
plot(head(sort(grocrules, by="support"), 20),
  method="graph", control=list(cex=.9))
```


#### This plot indicates the mapping from 20 different rules picked up from grocrules. As seen from the plot it is but clear that the whole milk, yogurt, other vegetables and root vegetables are almost at the nucleus of the plot indicating that they have a major role ot play in the association rule mappings. That is these prodcuts are picked up in combination with a lot of other different products with a higher probability. This plot also identifies those items which are not very well associated with the other items in the groceries basket. As for example the flour, pastry etc on the outer portion of the plot.


#### The plot gives more or less similar output when plotted by confidence or lift as parameter, as seen below. 


```{r}
plot(head(sort(grocrules, by="lift"), 20),
  method="graph", control=list(cex=.9))
```


### Hence to summarize:
 These are the interesting mapping we could obtain from the rules with choosing optimum support, lift and confidence    parameters
  
  1. Whole milk occurs with curd and yogurt with high confidence and lift values, indicating the set of people who are      regular buyers of dairy products.
  
  2. Root vegetables occurs with other vegetables, tropical fruits and citrus fruits indicating the set of people who       are very nutrient conscious and prefer mostly fruits and vegetables. It could also be possible that they are more      vegetarians as there is no significant association with these products and meat as observed.
  
  3. Vegetables occur a lot with whipped cream and sour cream indicating a category of people who enjoy the cream           products a lot have also higher chances of buying vegetables.
  
  4. Bottled beer occurs with liquor and red/blush wine with 90% confidence and a very high lift value of 11.               indicating the set of people who would buy beer with 11 times higher chance if they bought wine and liquor.
  
  
  